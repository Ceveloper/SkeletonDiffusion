{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "553b4359",
   "metadata": {},
   "source": [
    "# Perform inference on your data with SkeletonDiffusion\n",
    "\n",
    "If your data are in the same skeleton format as our trained model, you can perform inference from your data.\n",
    "Give a sequence of keypoints representing the past, and run SkeletonDiffusion to predict future motions!\n",
    "\n",
    "SkeletonDiffusion can run on the output of other models, for example methods for human pose estimation from images or video.\n",
    "For an example, check out our demo on Huggingface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f698f1",
   "metadata": {},
   "source": [
    "## Select model and data type\n",
    "Here we take as an example our model trained on AMASS, which follows the same parametrization (skeleton format) as SMPL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65d06fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose between 'amass' and 'amass-mano'\n",
    "# model_dataset = 'amass' \n",
    "model_dataset = 'amass-mano'\n",
    "\n",
    "checkpoint_path = f'./trained_models/hmp/{model_dataset}/diffusion/checkpoints/cvpr_release.pt'\n",
    "num_samples = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b643df",
   "metadata": {},
   "source": [
    "## Load model's weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e927d3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "from src.eval_prepare_model import prepare_model, get_prediction, load_model_config_exp\n",
    "from src.data import create_skeleton\n",
    "\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6ce7b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> GPU 0 ready: Quadro RTX 5000\n",
      "> GPU 1 ready: Quadro P400\n",
      "Loading Autoencoder checkpoint: ./trained_models/hmp/amass-mano/autoencoder/checkpoints/cvpr_release.pt ...\n",
      "Diffusion is_ddim_sampling:  False\n",
      "Loading Diffusion checkpoint: ./trained_models/hmp/amass-mano/diffusion/checkpoints/cvpr_release.pt ...\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed=0)\n",
    "\n",
    "config, exp_folder = load_model_config_exp(checkpoint_path)\n",
    "config['checkpoint_path'] = checkpoint_path\n",
    "skeleton = create_skeleton(**config)   \n",
    "\n",
    "\n",
    "model, device, *_ = prepare_model(config, skeleton, **config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae50633b",
   "metadata": {},
   "source": [
    "## Load given example or use your own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4aa1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare input\n",
    "# load input. Unit must be in meters\n",
    "# obs sequence contains the hip or root joint, it has not been dropped yet. \n",
    "obs = np.load(f'figures/example_obs_{model_dataset}.npy') # (t_past, J, 3)\n",
    "\n",
    "obs = torch.from_numpy(obs).to(device).float()\n",
    "# obs = obs.unsqueeze(0) # remember to add batch size if not present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e782b749",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_in = skeleton.tranform_to_input_space(obs) \n",
    "pred = get_prediction(obs_in, model, num_samples=num_samples, **config) # [batch_size, n_samples, seq_length, num_joints, features]\n",
    "pred = skeleton.transform_to_metric_space(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f91ee849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proceed to your own task.\n",
    "# For example, you can rank the output by the one with least limb stretching.\n",
    "# Checkout other metrics in src.metrics \n",
    "# or the diversity ranking in metrics/utils.py (see example in other notebook)\n",
    "\n",
    "\n",
    "from src.metrics.body_realism import limb_stretching_normed_mean\n",
    "\n",
    "\n",
    "limbstretching = limb_stretching_normed_mean(pred, target=obs[..., 1:, :][0].unsqueeze(1), limbseq=skeleton.get_limbseq(), reduction='persample', obs_as_target=True)\n",
    "limbstretching_sorted, indices =  torch.sort(limbstretching.squeeze(1), dim=-1, descending=False) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skeldiff4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
